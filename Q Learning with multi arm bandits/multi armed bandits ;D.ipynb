{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class contexual_bandit():\n",
    "    '''\n",
    "    makes 3 bandits and each bandits arm has its number that will have better chance of giving you reward \n",
    "    if it is lower\n",
    "    '''\n",
    "    def __init__(self,numB=3,numA=4):\n",
    "        self.state = 0\n",
    "        self.bandits = np.random.rand(numB,numA)\n",
    "        \n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "        \n",
    "    def getBandit(self):\n",
    "        self.state = np.random.randint(0,len(self.bandits))\n",
    "        \n",
    "        return self.state\n",
    "    def pullArm(self,action):\n",
    "        bandit = self.bandits[self.state,action]\n",
    "        res = np.random.rand(1)\n",
    "        \n",
    "        if(res < bandit):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    '''\n",
    "    an agent that will play with the arms and find which ones look the most lucrative\n",
    "    with diffrent types of implimentation you can use\n",
    "    '''\n",
    "    def __init__(self, lr, s_size, a_size,typeAgent=0,mean=0.1):\n",
    "        self.state_in = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        state_in_OH = slim.one_hot_encoding(self.state_in,s_size)\n",
    "        if(typeAgent==0):\n",
    "            output = slim.fully_connected(state_in_OH,a_size,biases_initializer=None,activation_fn=tf.nn.sigmoid,weights_initializer=tf.ones_initializer())\n",
    "        elif (typeAgent==1):\n",
    "            output = slim.fully_connected(state_in_OH,a_size,biases_initializer=None,activation_fn=tf.nn.softmax,weights_initializer=tf.ones_initializer())\n",
    "        elif (typeAgent==2):\n",
    "            output = slim.fully_connected(state_in_OH,a_size,biases_initializer=tf.truncated_normal_initializer(mean=1,stddev=1e-7),activation_fn=tf.nn.sigmoid,weights_initializer=tf.truncated_normal_initializer(mean=1,stddev=1e-5))\n",
    "        elif (typeAgent==3):\n",
    "            output = slim.fully_connected(state_in_OH,a_size,biases_initializer=tf.truncated_normal_initializer(mean=1,stddev=1e-7),activation_fn=tf.nn.softmax,weights_initializer=tf.truncated_normal_initializer(mean=1,stddev=1e-5))\n",
    "            \n",
    "        else:\n",
    "            middle = slim.fully_connected(state_in_OH,10,biases_initializer=tf.truncated_normal_initializer(mean=mean,stddev=2e-3),activation_fn=None,weights_initializer=tf.truncated_normal_initializer(mean=mean,stddev=2e-2))\n",
    "            output = slim.fully_connected(middle,a_size,biases_initializer=tf.truncated_normal_initializer(mean=mean,stddev=2e-3),activation_fn=tf.nn.sigmoid,weights_initializer=tf.truncated_normal_initializer(mean=mean,stddev=2e-2))\n",
    "\n",
    "        self.output = tf.reshape(output,[-1])\n",
    "        self.chosen_action = tf.arg_max(self.output,0)\n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n",
    "        \n",
    "        self.responsible_weight = tf.slice(self.output,self.action_holder,[1])\n",
    "        self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n",
    "        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n",
    "        self.update = self.optimizer.minimize(self.loss)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryAgent(episods=10000,maximumChance=0.8,minimumChance=0.1,toBePrinted=False,whichAgent=0,printLast=True,trueReward=1,nBandits=3,nActions=4,everyEps=500,basicShow=True):\n",
    "    tf.reset_default_graph()\n",
    "    # first let's make our env\n",
    "    cBandit = contexual_bandit(numB=nBandits,numA=nActions)\n",
    "    # and then our agent\n",
    "    myAgent = Agent(lr=0.001, s_size=cBandit.num_bandits, a_size=cBandit.num_actions,typeAgent=whichAgent)\n",
    "    trainables = tf.trainable_variables()\n",
    "    total_episodes = episods\n",
    "    total_reward = np.zeros(shape=[cBandit.num_bandits,cBandit.num_actions])\n",
    "    e=maximumChance\n",
    "    \n",
    "    init = tf.initialize_all_variables()\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        sess.run(init)\n",
    "        i=0\n",
    "        while i < total_episodes:\n",
    "            s = cBandit.getBandit()\n",
    "\n",
    "            if np.random.rand(1) < e :\n",
    "                action = np.random.randint(cBandit.num_actions)\n",
    "            else:\n",
    "                action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[s]})\n",
    "\n",
    "            reward = cBandit.pullArm(action)*trueReward\n",
    "\n",
    "            feed_dict={myAgent.state_in:[s],myAgent.action_holder:[action],myAgent.reward_holder:[reward]}\n",
    "\n",
    "            _ = sess.run([myAgent.update],feed_dict=feed_dict)\n",
    "\n",
    "\n",
    "            total_reward[s,action] += reward\n",
    "\n",
    "            if i%everyEps == 0:\n",
    "                if basicShow:\n",
    "                    print('mean of every one of the actions :'+str(np.mean(total_reward,axis=1)))\n",
    "                if i is not 0:\n",
    "                    e = min(max(e/(i/2000),minimumChance),maximumChance)\n",
    "                if(toBePrinted):\n",
    "                    for a in range(cBandit.num_bandits):\n",
    "                        action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[a]})\n",
    "                        print('the agent thinks the suitable action for this bandit is : '+str(action))\n",
    "                        if action == np.argmax(cBandit.bandits[a]):\n",
    "                            print('and it was right ...')\n",
    "                        else:\n",
    "                            print('and it was wrong ...')\n",
    "                        print()\n",
    "                    print\n",
    "            i+=1\n",
    "        per = 0\n",
    "        for a in range(cBandit.num_bandits):\n",
    "            \n",
    "            action = sess.run(myAgent.chosen_action,feed_dict={myAgent.state_in:[a]})\n",
    "            if printLast :\n",
    "                print('the agent thinks the suitable action for this bandit is : '+str(action))\n",
    "            if action == np.argmax(cBandit.bandits[a]):\n",
    "                per+=1\n",
    "                if printLast:\n",
    "                    print('and it was right ...')\n",
    "            else :\n",
    "                if printLast:\n",
    "                    print('and it was wrong ...')\n",
    "            if printLast:\n",
    "                print()\n",
    "        if printLast:\n",
    "            print(str((per/cBandit.num_bandits)*100)+\" percent of the time the agent works correctly\")\n",
    "        return (per/cBandit.num_bandits)*100\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of every one of the actions :[0.25 0.   0.  ]\n",
      "mean of every one of the actions :[ -7.75 -17.    -0.5 ]\n",
      "mean of every one of the actions :[-15.   -40.     1.75]\n",
      "mean of every one of the actions :[-19.   -65.75   2.  ]\n",
      "mean of every one of the actions :[-27.25 -93.5   10.5 ]\n",
      "mean of every one of the actions :[ -36.75 -117.75   14.75]\n",
      "mean of every one of the actions :[ -39.   -135.75   22.5 ]\n",
      "mean of every one of the actions :[ -32.25 -152.25   35.25]\n",
      "mean of every one of the actions :[ -22.5  -175.     47.25]\n",
      "mean of every one of the actions :[  -8.25 -189.75   54.75]\n",
      "mean of every one of the actions :[   8.   -206.25   58.  ]\n",
      "mean of every one of the actions :[  22.   -226.5    76.25]\n",
      "mean of every one of the actions :[  41.   -248.5    95.25]\n",
      "mean of every one of the actions :[  60.25 -276.    109.  ]\n",
      "mean of every one of the actions :[  76.5  -298.5   122.75]\n",
      "mean of every one of the actions :[  90.75 -322.25  139.75]\n",
      "mean of every one of the actions :[ 103.   -342.75  153.5 ]\n",
      "mean of every one of the actions :[ 117.   -369.    163.75]\n",
      "mean of every one of the actions :[ 134.   -384.25  176.5 ]\n",
      "mean of every one of the actions :[ 147.   -406.    190.75]\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "100.0 percent of the time the agent works correctly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tryAgent(episods=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will try to get an average accuracy for each agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the 0'th batch of bandits\n",
      "in the 5'th batch of bandits\n",
      "in the 10'th batch of bandits\n",
      "in the 15'th batch of bandits\n",
      "in the 20'th batch of bandits\n",
      "in the 25'th batch of bandits\n",
      "in the 30'th batch of bandits\n",
      "in the 35'th batch of bandits\n",
      "in the 40'th batch of bandits\n",
      "in the 45'th batch of bandits\n",
      "in the 50'th batch of bandits\n",
      "in the 55'th batch of bandits\n",
      "in the 60'th batch of bandits\n",
      "in the 65'th batch of bandits\n",
      "in the 70'th batch of bandits\n",
      "in the 75'th batch of bandits\n"
     ]
    }
   ],
   "source": [
    "res = [[] for i in range(5)]\n",
    "for i in range(80):\n",
    "    if i%5==0:\n",
    "        print(\"in the %d'th batch of bandits\"%i)\n",
    "    np.random.seed(i)\n",
    "    for j in range(5):\n",
    "        res[j].append(tryAgent(whichAgent=j,episods=10000,nBandits=10,nActions=5,basicShow=False,printLast=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason the first two agents do so much better than the others is that the latter agents<br>\n",
    "may have a weight and a bias that are not equal in their output cells that will make them think they need to make an action<br>\n",
    "they don't where as the first two agents every action has the same activation in the last layer<br>\n",
    "so for the latter agents we need a lot of more random action (for exploration) and therefor a lot of more episodes :D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the average accuracy for the 0'th agent was : 77.875\n",
      "the average accuracy for the 1'th agent was : 76.75\n",
      "the average accuracy for the 2'th agent was : 36.0\n",
      "the average accuracy for the 3'th agent was : 37.875\n",
      "the average accuracy for the 4'th agent was : 33.375\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"the average accuracy for the \"+str(i)+\"'th agent was : \"+str(sum(res[i])/80))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from here there are some examples for you to see how to use the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of every one of the actions :[0.  0.  0.  0.  0.  0.  0.2 0.  0.  0. ]\n",
      "mean of every one of the actions :[ 1.4 -4.   2.2  5.  -3.4  3.6  1.4 -2.   3.   2.2]\n",
      "mean of every one of the actions :[ 6.4 -7.8  7.   7.8 -5.2  7.2  5.   0.   4.8  5.8]\n",
      "mean of every one of the actions :[ 9.4 -9.  12.6 11.2 -6.   7.6  6.6 -0.2  5.6  5.6]\n",
      "mean of every one of the actions :[  8.8 -12.   15.6  13.   -7.2   8.4   9.2  -1.8   6.6   7.2]\n",
      "mean of every one of the actions :[ 11.2 -14.8  17.   16.2  -5.4   9.8  10.2  -2.8   9.4   7.8]\n",
      "mean of every one of the actions :[ 15.  -14.   21.2  21.2  -7.8  12.2  15.4  -1.8  11.4  11. ]\n",
      "mean of every one of the actions :[ 18.2 -13.   29.   23.2  -8.   15.   17.2   0.6  13.6  15.6]\n",
      "mean of every one of the actions :[ 22.8 -10.   38.   27.8  -7.   16.6  21.2   5.8  15.6  21.4]\n",
      "mean of every one of the actions :[27.8 -7.4 47.8 29.6 -5.8 18.4 25.2  9.  21.  24.2]\n",
      "mean of every one of the actions :[34.6 -2.6 54.6 35.8 -1.2 19.6 31.2 13.  25.  27.4]\n",
      "mean of every one of the actions :[41.2  0.8 63.4 37.8  0.4 22.8 35.2 17.2 31.8 34. ]\n",
      "mean of every one of the actions :[44.6  3.8 70.6 39.   3.8 23.6 42.2 22.4 35.2 41.8]\n",
      "mean of every one of the actions :[49.2  6.2 77.8 46.4  7.  26.  46.8 26.6 39.8 48.4]\n",
      "mean of every one of the actions :[54.8  9.  83.4 50.6 10.4 29.  54.  31.8 44.2 53.4]\n",
      "mean of every one of the actions :[58.6 11.  90.2 56.4 13.8 33.2 57.2 38.  48.6 57.6]\n",
      "mean of every one of the actions :[63.6 13.4 99.2 63.4 14.  37.6 60.  45.  53.6 64.4]\n",
      "mean of every one of the actions :[ 69.8  17.4 106.2  67.   17.4  39.2  65.2  52.2  59.6  70.2]\n",
      "mean of every one of the actions :[ 75.   20.8 114.8  69.4  21.4  41.6  71.6  58.   64.   75.6]\n",
      "mean of every one of the actions :[ 79.6  26.2 119.4  72.4  21.2  44.   76.   62.8  67.8  82.4]\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 2\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 2\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 2\n",
      "and it was right ...\n",
      "\n",
      "70.0 percent of the time the agent works correctly\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "tryAgent(whichAgent=1,episods=10000,nBandits=10,nActions=5,minimumChance=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of every one of the actions :[ 0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0. ]\n",
      "mean of every one of the actions :[-5.2 -5.6  0.2 -0.2 10.6  1.8  6.4 -0.2  6.4  0.8]\n",
      "mean of every one of the actions :[-10.8  -9.6  -0.8  -4.4  20.    5.4  13.4   2.6  11.4   0.2]\n",
      "mean of every one of the actions :[-16.4 -15.8  -0.4  -7.6  30.8   7.   18.    4.8  14.8  -0.6]\n",
      "mean of every one of the actions :[-19.2 -17.    5.6  -7.2  45.   10.   26.8   8.2  25.8   4.6]\n",
      "mean of every one of the actions :[-18.4 -20.2  14.4  -3.6  56.   22.2  42.2  16.   39.6  17.2]\n",
      "mean of every one of the actions :[-10.4 -19.   25.2   1.   71.6  34.2  59.8  25.4  61.6  33.2]\n",
      "mean of every one of the actions :[ -2.4 -16.2  39.6   5.4  85.8  50.4  76.4  34.   81.2  46.4]\n",
      "mean of every one of the actions :[  6.  -15.2  52.6  14.4 103.   63.   91.8  44.8  97.6  62.2]\n",
      "mean of every one of the actions :[ 14.6 -12.6  65.8  16.4 118.4  74.6 105.2  53.4 117.2  76. ]\n",
      "mean of every one of the actions :[ 18.2 -12.6  79.8  20.6 137.4  87.6 122.2  61.  138.   89.6]\n",
      "mean of every one of the actions :[ 26.2 -12.4  90.8  26.4 156.4 101.8 139.8  69.2 157.2 104.8]\n",
      "mean of every one of the actions :[ 29.2 -12.6 106.8  33.  173.4 116.4 156.2  77.6 176.  123.4]\n",
      "mean of every one of the actions :[ 38.8  -9.2 121.6  44.  191.2 131.  172.6  88.2 197.4 137.4]\n",
      "mean of every one of the actions :[ 44.8  -9.2 134.4  50.4 208.4 146.8 191.4  95.  216.6 153.2]\n",
      "mean of every one of the actions :[ 56.2  -7.6 149.8  57.8 227.  161.2 206.8 100.8 237.4 168. ]\n",
      "mean of every one of the actions :[ 63.8  -4.  163.   64.  246.6 175.2 222.8 107.8 257.4 180.4]\n",
      "mean of every one of the actions :[ 71.    1.8 177.8  70.6 263.2 185.8 238.8 115.4 272.6 194.8]\n",
      "mean of every one of the actions :[ 80.2  -1.2 189.2  78.2 283.8 199.8 256.8 126.4 289.6 209. ]\n",
      "mean of every one of the actions :[ 88.2  -2.4 204.2  87.2 300.8 215.8 276.2 133.6 307.2 227.8]\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 4\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 2\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 4\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "90.0 percent of the time the agent works correctly\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tryAgent(whichAgent=1,episods=20000,nBandits=10,nActions=5,everyEps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of every one of the actions :[ 0.   0.   0.   0.   0.  -0.2  0.   0.   0.   0. ]\n",
      "mean of every one of the actions :[-10.8  -9.6  -0.8  -4.4  20.    5.4  13.4   2.6  11.4   0.2]\n",
      "mean of every one of the actions :[-20.2 -22.   -0.6  -9.6  42.8   9.2  25.2   5.8  17.8  -2.2]\n",
      "mean of every one of the actions :[-14.4 -26.8  18.4  -6.4  63.   25.2  48.8  21.8  41.6  19. ]\n",
      "mean of every one of the actions :[  0.8 -26.8  43.2   3.6  95.   50.   79.6  40.6  76.2  46.8]\n",
      "mean of every one of the actions :[ 10.4 -23.   71.8  10.  128.2  75.8 110.6  57.2 116.6  76.6]\n",
      "mean of every one of the actions :[ 23.4 -23.6  98.4  23.  164.4 104.4 144.4  76.  156.  110.6]\n",
      "mean of every one of the actions :[ 39.8 -20.  124.4  40.6 200.  134.4 179.   91.6 194.4 139.6]\n",
      "mean of every one of the actions :[ 57.  -14.6 154.   53.6 239.  164.  210.2 105.6 233.6 165.8]\n",
      "mean of every one of the actions :[ 73.  -13.6 179.8  66.4 276.4 188.  244.6 122.8 266.4 196.4]\n",
      "mean of every one of the actions :[ 88.6 -13.4 210.2  81.6 310.4 223.  281.4 138.4 303.6 225.6]\n",
      "mean of every one of the actions :[106.4 -18.6 240.2  94.  341.4 252.4 306.8 156.  341.4 253.4]\n",
      "mean of every one of the actions :[121.2 -14.6 264.2 105.  378.4 279.  342.4 177.6 380.8 281.8]\n",
      "mean of every one of the actions :[136.2 -15.8 291.2 116.8 411.2 307.4 378.4 198.2 413.  308.4]\n",
      "mean of every one of the actions :[156.2  -9.8 318.8 128.  450.  336.6 413.6 216.6 453.8 334.4]\n",
      "mean of every one of the actions :[166.4  -7.2 347.4 140.8 486.4 366.8 453.6 234.6 490.4 361.4]\n",
      "mean of every one of the actions :[181.6  -6.4 373.6 154.2 524.8 396.8 483.4 254.6 530.  388. ]\n",
      "mean of every one of the actions :[195.4  -3.6 403.2 171.2 553.8 425.2 513.8 274.8 566.  416.8]\n",
      "mean of every one of the actions :[211.2  -3.4 432.2 182.  583.4 456.4 541.2 294.4 606.2 443.4]\n",
      "mean of every one of the actions :[218.6   1.4 461.8 195.4 617.2 482.6 575.6 310.8 643.2 472.4]\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 4\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 3\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 2\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 4\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 0\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "90.0 percent of the time the agent works correctly\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tryAgent(whichAgent=1,episods=40000,nBandits=10,nActions=5,everyEps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean of every one of the actions :[ 0.  -0.2  0.   0. ]\n",
      "mean of every one of the actions :[-20.2 -41.2  -5.6 -19.2]\n",
      "mean of every one of the actions :[-38.  -83.8  -2.4 -35.6]\n",
      "mean of every one of the actions :[ -31.4 -129.2   23.   -57.4]\n",
      "mean of every one of the actions :[   2.6 -171.2   49.4  -75. ]\n",
      "mean of every one of the actions :[  37.  -218.4   85.4  -82.2]\n",
      "mean of every one of the actions :[  73.6 -257.8  123.4  -88.6]\n",
      "mean of every one of the actions :[ 113.  -284.   154.6  -97.4]\n",
      "mean of every one of the actions :[ 148.  -304.4  191.2 -113.4]\n",
      "mean of every one of the actions :[ 186.8 -333.6  225.8 -134.4]\n",
      "mean of every one of the actions :[ 220.8 -351.2  256.4 -149.4]\n",
      "mean of every one of the actions :[ 250.6 -364.   280.6 -163.8]\n",
      "mean of every one of the actions :[ 281.  -384.   316.2 -181.8]\n",
      "mean of every one of the actions :[ 316.8 -392.   344.  -202.6]\n",
      "mean of every one of the actions :[ 358.  -408.   376.2 -216.8]\n",
      "mean of every one of the actions :[ 386.6 -400.6  405.4 -224.4]\n",
      "mean of every one of the actions :[ 425.4 -412.8  439.6 -238.4]\n",
      "mean of every one of the actions :[ 460.2 -416.4  477.2 -258.8]\n",
      "mean of every one of the actions :[ 488.6 -418.2  505.  -276. ]\n",
      "mean of every one of the actions :[ 519.2 -428.   542.4 -294.6]\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 4\n",
      "and it was right ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was wrong ...\n",
      "\n",
      "the agent thinks the suitable action for this bandit is : 1\n",
      "and it was wrong ...\n",
      "\n",
      "50.0 percent of the time the agent works correctly\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "tryAgent(whichAgent=2,episods=40000,nBandits=4,nActions=5,everyEps=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
