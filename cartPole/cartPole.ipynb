{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\pooria\\anaconda3\\envs\\myonlyenv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "def discounted(r):\n",
    "    res = np.zeros_like(r)\n",
    "    temp = 0\n",
    "    for i in reversed(range(0,r.size)):\n",
    "        temp = temp*gamma + r[i]\n",
    "        res[i] = temp\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([166, 168, 169, 168, 167, 165, 161, 157, 152, 145, 137, 129, 119,\n",
       "       108,  96,  83,  68,  53,  36,  19])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discounted(np.arange(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, lr, s_size, a_size, h_size):\n",
    "        self.state_in = tf.placeholder(shape=[None, s_size],dtype=tf.float32)\n",
    "        self.hidden = slim.fully_connected(self.state_in,h_size,biases_initializer=None,activation_fn=tf.nn.relu)\n",
    "        self.output = slim.fully_connected(self.hidden,a_size,activation_fn=tf.nn.softmax)\n",
    "        \n",
    "        self.chosen_action = tf.argmax(self.output,1)\n",
    "        \n",
    "        \n",
    "        self.reward_holder = tf.placeholder(shape=[None],dtype=tf.float32)\n",
    "        self.action_holder = tf.placeholder(shape=[None],dtype=tf.int32)\n",
    "        \n",
    "        self.indexes = tf.range(0,tf.shape(self.output)[0])*tf.shape(self.output)[1] + self.action_holder\n",
    "        \n",
    "        self.responsible_weights = tf.gather(tf.reshape(self.output,shape=[-1]),self.indexes)\n",
    "        \n",
    "        self.loss = - tf.reduce_mean(tf.log(self.responsible_weights)*self.reward_holder)\n",
    "        \n",
    "        \n",
    "        t_vars = tf.trainable_variables()\n",
    "        \n",
    "        self.gradiant_holders = []\n",
    "        \n",
    "        for idx,par in enumerate(t_vars):\n",
    "            placeholder = tf.placeholder(tf.float32,name=str(idx)+\"_holder\")\n",
    "            self.gradiant_holders.append(placeholder)\n",
    "#         the next lines result should go in to the place holders we made and then those will be used to update\n",
    "        self.gradiant = tf.gradients(self.loss,t_vars)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "        self.update_paramaters = self.optimizer.apply_gradients(zip(self.gradiant_holders,t_vars))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tryAgents(total_episodes = 5000, max_per = 999, lr=1e-2, h_size=8, freq = 5, afterWhichEpisode = None, paintEnv=True, showReward=True):\n",
    "    tf.reset_default_graph()\n",
    "    myAgent = Agent(lr, 4, 2, h_size)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    if afterWhichEpisode == None :\n",
    "        afterWhichEpisode = int((total_episodes*3)/4)\n",
    "    show = False\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        sess.run(init)\n",
    "        gradient_buffer = sess.run(tf.trainable_variables())\n",
    "        i = 0\n",
    "        total_rewards =[]\n",
    "        total_length = []\n",
    "\n",
    "\n",
    "        for idx,grad in enumerate(gradient_buffer):\n",
    "            gradient_buffer[idx] = grad * 0\n",
    "        while i < total_episodes :\n",
    "            if paintEnv and i > afterWhichEpisode :\n",
    "                show = True\n",
    "            history = []\n",
    "            rewardTillNow = 0\n",
    "            s = env.reset()\n",
    "            if show :\n",
    "                env.render()\n",
    "            for j in range (total_episodes):\n",
    "                a_dist = sess.run(myAgent.output,feed_dict={myAgent.state_in:[s]})\n",
    "                a = np.random.choice(a_dist[0],p=a_dist[0])\n",
    "                a = np.argmax(a_dist == a)\n",
    "\n",
    "\n",
    "\n",
    "                s1,r,d,_ = env.step(a)\n",
    "                if show :\n",
    "                    env.render()\n",
    "\n",
    "                history.append([s,a,r,s1])\n",
    "\n",
    "                rewardTillNow += r\n",
    "\n",
    "                s = s1\n",
    "\n",
    "                if d is True :\n",
    "                    history = np.array(history)\n",
    "                    history[:,2] = discounted(history[:,2])\n",
    "                    feed_dict = {myAgent.reward_holder:history[:,2],myAgent.action_holder:history[:,1],myAgent.state_in:np.vstack(history[:,0])}\n",
    "                    grads = sess.run(myAgent.gradiant,feed_dict=feed_dict)\n",
    "                    for idx,grad in enumerate(grads):\n",
    "                        gradient_buffer[idx] += grad\n",
    "\n",
    "                    if i%freq == 0 and i != 0:\n",
    "                        feed_dict = dict(zip(myAgent.gradiant_holders,gradient_buffer))\n",
    "                        _ = sess.run(myAgent.update_paramaters,feed_dict=feed_dict)\n",
    "                        for idx,grad in enumerate(gradient_buffer):\n",
    "                            gradient_buffer[idx] = grad*0\n",
    "                    total_rewards.append(rewardTillNow)\n",
    "                    total_length.append(j)\n",
    "                    break\n",
    "\n",
    "            i+=1\n",
    "            if i%100 == 0 and i != 0 and showReward :\n",
    "                print(str(i//100)+\" : the average reward for the last 100 episodes : \"+str(np.mean(total_rewards[-100:])))\n",
    "        return total_rewards,total_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\pooria\\anaconda3\\envs\\myonlyenv\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : the average reward for the last 100 episodes : 30.84\n",
      "2 : the average reward for the last 100 episodes : 33.46\n",
      "3 : the average reward for the last 100 episodes : 41.86\n",
      "4 : the average reward for the last 100 episodes : 49.08\n",
      "5 : the average reward for the last 100 episodes : 72.36\n",
      "6 : the average reward for the last 100 episodes : 87.38\n",
      "7 : the average reward for the last 100 episodes : 136.21\n",
      "8 : the average reward for the last 100 episodes : 166.32\n",
      "9 : the average reward for the last 100 episodes : 188.63\n",
      "10 : the average reward for the last 100 episodes : 194.66\n",
      "11 : the average reward for the last 100 episodes : 189.29\n",
      "12 : the average reward for the last 100 episodes : 193.32\n",
      "13 : the average reward for the last 100 episodes : 198.92\n",
      "14 : the average reward for the last 100 episodes : 198.54\n",
      "15 : the average reward for the last 100 episodes : 198.48\n",
      "16 : the average reward for the last 100 episodes : 194.51\n",
      "17 : the average reward for the last 100 episodes : 195.14\n",
      "18 : the average reward for the last 100 episodes : 190.98\n",
      "19 : the average reward for the last 100 episodes : 197.26\n",
      "20 : the average reward for the last 100 episodes : 199.73\n",
      "21 : the average reward for the last 100 episodes : 198.7\n",
      "22 : the average reward for the last 100 episodes : 196.76\n",
      "23 : the average reward for the last 100 episodes : 198.93\n",
      "24 : the average reward for the last 100 episodes : 198.85\n",
      "25 : the average reward for the last 100 episodes : 198.82\n",
      "26 : the average reward for the last 100 episodes : 200.0\n",
      "27 : the average reward for the last 100 episodes : 200.0\n",
      "28 : the average reward for the last 100 episodes : 200.0\n",
      "29 : the average reward for the last 100 episodes : 200.0\n",
      "30 : the average reward for the last 100 episodes : 200.0\n",
      "31 : the average reward for the last 100 episodes : 198.75\n",
      "32 : the average reward for the last 100 episodes : 198.44\n",
      "33 : the average reward for the last 100 episodes : 200.0\n",
      "34 : the average reward for the last 100 episodes : 199.34\n",
      "35 : the average reward for the last 100 episodes : 199.73\n",
      "36 : the average reward for the last 100 episodes : 198.63\n",
      "37 : the average reward for the last 100 episodes : 196.37\n",
      "38 : the average reward for the last 100 episodes : 192.41\n",
      "39 : the average reward for the last 100 episodes : 193.16\n",
      "40 : the average reward for the last 100 episodes : 192.78\n"
     ]
    }
   ],
   "source": [
    "# if you want to see whats happening in the env turn the paintEnv arg to True but it'll take more time to train\n",
    "tr,tl = tryAgents(total_episodes=4000,paintEnv=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the longest episode of actions is : 199\n"
     ]
    }
   ],
   "source": [
    "print('the longest episode of actions is : '+str(max(tl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
